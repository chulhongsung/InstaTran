{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.abspath('')))\n",
    "sys.path.append(os.path.abspath(os.path.abspath('') + '/src'))\n",
    "os.chdir(os.path.abspath(''))\n",
    "\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mead = pd.read_csv(\"../data/us_lakes/df_mead_preprocessed.csv\")\n",
    "df_mohave = pd.read_csv(\"../data/us_lakes/df_mohave_preprocessed.csv\")\n",
    "df_havasu = pd.read_csv(\"../data/us_lakes/df_havasu_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split_for_dl(df_mead, df_mohave, df_havasu, valid_size=2/9, test_size=1/3, input_seq_len=24, tau=4):\n",
    "    N, _ = df_mohave.shape\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    \n",
    "    df_mead.columns = [\"e1\", \"i1\", \"o1\", \"y\", \"m\", \"d\"]\n",
    "    df_mohave.columns = [\"e2\", \"i2\", \"o2\", \"y\", \"m\", \"d\", \"p1\"]\n",
    "    df_havasu.columns = [\"e3\", \"i3\", \"o3\", \"y\", \"m\", \"d\", \"p2\"]\n",
    "    \n",
    "    df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
    "    df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
    "    df_havasu.drop(columns=[\"y\"], inplace=True)\n",
    "    \n",
    "    df_havasu_date = df_havasu.loc[:, [\"m\", \"d\"]]\n",
    "    df_havasu_cov = df_havasu.loc[:, [\"e3\", \"i3\", \"o3\", \"p2\"]]\n",
    "    \n",
    "    index_1 = round(N * (1 - valid_size - test_size))\n",
    "    index_2 = round(N * (1-test_size))\n",
    "    \n",
    "    df_mead_train = df_mead.iloc[:index_1, :]\n",
    "    df_mohave_train = df_mohave.iloc[:index_1, :]\n",
    "    df_havasu_train = df_havasu_cov.iloc[:index_1, :]\n",
    "    df_train_date = df_havasu_date.iloc[:index_1, :]\n",
    "    \n",
    "    df_mead_valid = df_mead.iloc[index_1:index_2, :]\n",
    "    df_mohave_valid = df_mohave.iloc[index_1:index_2, :]\n",
    "    df_havasu_valid = df_havasu_cov.iloc[index_1:index_2, :]\n",
    "    df_valid_date = df_havasu_date.iloc[index_1:index_2, :]\n",
    "\n",
    "    df_mead_test = df_mead.iloc[index_2:, :]\n",
    "    df_mohave_test = df_mohave.iloc[index_2:, :]\n",
    "    df_havasu_test = df_havasu_cov.iloc[index_2:, :]\n",
    "    df_test_date = df_havasu_date.iloc[index_2:, :]\n",
    "    \n",
    "    df_train = pd.concat([df_mead_train, df_mohave_train, df_havasu_train], axis=1)\n",
    "    df_valid = pd.concat([df_mead_valid, df_mohave_valid, df_havasu_valid], axis=1)\n",
    "    df_test = pd.concat([df_mead_test, df_mohave_test, df_havasu_test], axis=1)\n",
    "    \n",
    "    imp_mean.fit(df_train)\n",
    "    df_train = imp_mean.transform(df_train)\n",
    "    df_valid = imp_mean.transform(df_valid)\n",
    "    df_test = imp_mean.transform(df_test)\n",
    "    \n",
    "    col_labels = 7\n",
    "    \n",
    "    tmp_arr = np.array(df_train)\n",
    "    tmp_label = np.array(df_train[:, col_labels])\n",
    "    tmp_date = np.array(df_train_date)\n",
    "\n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    train_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    train_label = np.zeros((n, tau))\n",
    "    train_date = np.zeros((n, input_seq_len, 2))\n",
    "    train_future = np.zeros((n, tau, 2))\n",
    "        \n",
    "    for j in range(n):\n",
    "        train_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        train_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        train_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "        train_future[j, :] = tmp_date[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "        \n",
    "    tmp_arr = np.array(df_valid)\n",
    "    tmp_label = np.array(df_valid[:, col_labels])\n",
    "    tmp_date = np.array(df_valid_date)\n",
    "    \n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    valid_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    valid_label = np.zeros((n, tau))\n",
    "    valid_date = np.zeros((n, input_seq_len, 2))\n",
    "    valid_future = np.zeros((n, tau, 2))\n",
    "    \n",
    "    for j in range(n):\n",
    "        valid_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        valid_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        valid_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "        valid_future[j, :] = tmp_date[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "        \n",
    "    tmp_arr = np.array(df_test)\n",
    "    tmp_label = np.array(df_test[:, col_labels])\n",
    "    tmp_date = np.array(df_test_date)\n",
    "    \n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    test_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    test_label = np.zeros((n, tau))\n",
    "    test_date = np.zeros((n, input_seq_len, 2))\n",
    "    test_future = np.zeros((n, tau, 2))\n",
    "    \n",
    "    for j in range(n):\n",
    "        test_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        test_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        test_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "        test_future[j, :] = tmp_date[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "    \n",
    "    scaler.fit(train_input.reshape(train_input.shape[0], -1))\n",
    "    train_scaled = scaler.transform(train_input.reshape(train_input.shape[0], -1)).reshape(train_input.shape[0], input_seq_len, -1)\n",
    "    valid_scaled = scaler.transform(valid_input.reshape(valid_input.shape[0], -1)).reshape(valid_input.shape[0], input_seq_len, -1)\n",
    "    test_scaled = scaler.transform(test_input.reshape(test_input.shape[0], -1)).reshape(test_input.shape[0], input_seq_len, -1)\n",
    "    \n",
    "    return (train_scaled, train_date, train_future, train_label), (valid_scaled, valid_date, valid_future, valid_label), (test_scaled, test_date, test_future, test_label), scaler\n",
    "\n",
    "# %%\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_input, d_embedding, n_embedding, d_model, n_layers=3, dr=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(n, d_embedding) for n in n_embedding]) \n",
    "        self.lstm = nn.LSTM(d_input + len(n_embedding) * d_embedding, d_model, n_layers, dropout=dr, batch_first=True)\n",
    "        \n",
    "    def forward(self, conti, cate):\n",
    "        tmp_feature_list = []\n",
    "        \n",
    "        for i, l in enumerate(self.embedding_layers):\n",
    "            tmp_feature = l(cate[:, :, i:i+1])\n",
    "            tmp_feature_list.append(tmp_feature)\n",
    "            \n",
    "        emb_output = torch.cat(tmp_feature_list, axis=-2)\n",
    "        emb_output = emb_output.view(conti.size(0), conti.size(1), -1)\n",
    "        \n",
    "        x = torch.cat([conti, emb_output], axis=-1)\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class GlobalDecoder(nn.Module):\n",
    "    def __init__(self, d_hidden:int, d_embedding:int, n_embedding:list, d_model:int, tau:int, num_targets:int, dr:float):\n",
    "        super(GlobalDecoder, self).__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_embedding = d_embedding\n",
    "        self.n_embedding = n_embedding\n",
    "        self.d_model = d_model\n",
    "        self.tau = tau\n",
    "        self.num_targets = num_targets\n",
    "        self.dr = dr\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(n, d_embedding) for n in n_embedding]) \n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_hidden + tau * d_embedding * len(n_embedding), (tau+1) * d_model) for _ in range(num_targets)])\n",
    "        self.dropout = nn.Dropout(dr)\n",
    "        \n",
    "    def forward(self, future, hidden):\n",
    "        tmp_feature_list = []\n",
    "        \n",
    "        for i, l in enumerate(self.embedding_layers):\n",
    "            tmp_feature = l(future[:, :, i:i+1])\n",
    "            tmp_feature_list.append(tmp_feature)\n",
    "            \n",
    "        emb_output_ = torch.cat(tmp_feature_list, axis=-2)\n",
    "        emb_output = emb_output_.view(future.size(0), -1)\n",
    "        \n",
    "        num_layers, batch_size, d_hidden = hidden.size()\n",
    "        \n",
    "        assert d_hidden == self.d_model \n",
    "        \n",
    "        x = torch.cat([hidden[num_layers-1], emb_output], axis=-1)\n",
    "        \n",
    "        tmp_global_context = []\n",
    "        for l in self.linear_layers:\n",
    "            tmp_gc = self.dropout(l(x))\n",
    "            tmp_global_context.append(tmp_gc.unsqueeze(1))\n",
    "        \n",
    "        global_context = torch.cat(tmp_global_context, axis=1)\n",
    "        \n",
    "        return emb_output_.view(batch_size, self.tau, -1), global_context # (batch_size, tau, d_embedding * len(n_embedding)), (batch_size, num_targets, (tau+1) * d_model), (tau+1): c_{a} , c_{t+1:t+tau}\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    def __init__(self, d_hidden:int, d_embedding:int, n_embedding: list, d_model:int, tau:int, num_targets:int, num_quantiles:int, dr:float):\n",
    "        super(LocalDecoder, self).__init__()\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_embedding = d_embedding\n",
    "        self.n_embedding = n_embedding\n",
    "        self.d_model = d_model\n",
    "        self.tau = tau\n",
    "        self.num_targets = num_targets\n",
    "        self.dr = dr\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(2 * d_model + d_embedding * len(n_embedding), d_model * 2),\n",
    "            nn.Dropout(dr),\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.Dropout(dr),\n",
    "            nn.Linear(d_model, num_quantiles)            \n",
    "            )\n",
    "                \n",
    "    def forward(self, embedded_future, global_output):\n",
    "        batch_size = global_output.size(0)\n",
    "        \n",
    "        c_a = global_output[..., :self.d_model].unsqueeze(-2).repeat(1, 1, self.tau, 1) # (batch_size, num_targets, tau, d_model)\n",
    "        c_t = global_output[..., self.d_model:].view(batch_size, self.num_targets, self.tau, -1) # (batch_size, num_targets, tau, d_model)\n",
    "        x_ = torch.cat([c_a,c_t.view(batch_size, self.num_targets, self.tau, -1)], axis=-1) # (batch_size, num_targets, tau, 2*d_model)\n",
    "        x = torch.cat([x_, embedded_future.unsqueeze(1).repeat(1, self.num_targets, 1, 1)], axis=-1) # (batch_size, num_targets, tau, 2*d_model + d_embedding * len(n_embedding))\n",
    "        \n",
    "        output = self.linear_layers(x)\n",
    "        \n",
    "        return output.transpose(2, 1).squeeze() # (batch_size, tau, num_targets, num_quantiles)\n",
    "    \n",
    "class MQRnn(nn.Module):\n",
    "    def __init__(self, d_input:int, d_embedding:int, n_embedding:list, d_model:int, tau:int, num_targets:int, num_quantiles: int, n_layers:int, dr:float):\n",
    "        super(MQRnn, self).__init__()\n",
    "        self.encoder = Encoder(\n",
    "                               d_input=d_input,\n",
    "                               d_embedding=d_embedding,\n",
    "                               n_embedding=n_embedding,\n",
    "                               d_model=d_model,\n",
    "                               n_layers=n_layers,\n",
    "                               dr=dr\n",
    "                               )\n",
    "        self.global_decoder = GlobalDecoder(\n",
    "                                            d_hidden=d_model,\n",
    "                                            d_embedding=d_embedding,\n",
    "                                            n_embedding=n_embedding,\n",
    "                                            d_model=d_model,\n",
    "                                            tau=tau,\n",
    "                                            num_targets=num_targets,\n",
    "                                            dr=dr\n",
    "                                            )\n",
    "        self.local_decoder = LocalDecoder(\n",
    "                                          d_hidden=d_model,\n",
    "                                          d_embedding=d_embedding,\n",
    "                                          n_embedding=n_embedding,\n",
    "                                          d_model=d_model,\n",
    "                                          tau=tau,\n",
    "                                          num_targets=num_targets,\n",
    "                                          num_quantiles=num_quantiles,\n",
    "                                          dr=dr\n",
    "                                          )\n",
    "        \n",
    "    def forward(self, conti, cate, future):\n",
    "        hidden, _ = self.encoder(conti, cate)\n",
    "        embedded_future, global_output = self.global_decoder(future, hidden)\n",
    "        output = self.local_decoder(embedded_future, global_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        conti_input, cate_input, future_input, true_y = batch \n",
    "        \n",
    "        conti_input = conti_input.to(device)\n",
    "        cate_input = cate_input.to(device)\n",
    "        future_input = future_input.to(device)\n",
    "        true_y = true_y.to(device)\n",
    "        \n",
    "        pred = model(conti_input, cate_input, future_input)\n",
    "        \n",
    "        loss = criterion(true_y, pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss)\n",
    "        \n",
    "    return sum(total_loss)/len(total_loss)\n",
    "\n",
    "class QuantileRisk(nn.Module):\n",
    "    def __init__(self, tau, quantile, device):\n",
    "        super(QuantileRisk, self).__init__()\n",
    "        self.quantile = quantile\n",
    "        self.device = device\n",
    "        self.q_arr = torch.tensor(quantile).float().unsqueeze(-1).repeat(1, 1, tau).transpose(-1, -2).to(self.device)\n",
    "    \n",
    "    def forward(self, true, pred):\n",
    "        \n",
    "        ql = torch.maximum(self.q_arr * (true.unsqueeze(-1) - pred), (1-self.q_arr)*(pred - true.unsqueeze(-1)))\n",
    "\n",
    "        return ql.mean() * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71214/1894844140.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "Train Loss: 0.1154: 100%|██████████| 600/600 [00:22<00:00, 26.99it/s]\n",
      "/tmp/ipykernel_71214/1894844140.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "Train Loss: 0.1210: 100%|██████████| 600/600 [00:22<00:00, 27.08it/s]\n",
      "/tmp/ipykernel_71214/1894844140.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "Train Loss: 0.1249: 100%|██████████| 600/600 [00:22<00:00, 27.05it/s]\n",
      "/tmp/ipykernel_71214/1894844140.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_71214/1894844140.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "Train Loss: 0.0985: 100%|██████████| 600/600 [00:22<00:00, 27.09it/s]\n"
     ]
    }
   ],
   "source": [
    "data_split_range = [(2005, 2013), (2008, 2016), (2011, 2019), (2014, 2022)]\n",
    "\n",
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test, scaler = train_valid_test_split_for_dl(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "     \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(tmp_train[0]), torch.LongTensor(tmp_train[1]), torch.LongTensor(tmp_train[2]), torch.FloatTensor(tmp_train[3]))\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=256)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MQRnn(\n",
    "        d_input=11, \n",
    "        d_embedding=1, \n",
    "        n_embedding=[13, 32], \n",
    "        d_model=20, \n",
    "        tau=4,\n",
    "        num_targets=1,\n",
    "        num_quantiles=5,\n",
    "        n_layers=3,\n",
    "        dr=0.05\n",
    "    )\n",
    "        \n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = QuantileRisk(4, [0.1, 0.3, 0.5, 0.7, 0.9], device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.003)\n",
    "    \n",
    "    pbar = tqdm(range(600))\n",
    "\n",
    "    for epoch in pbar:        \n",
    "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        pbar.set_description(\"Train Loss: {:.4f}\".format(train_loss))\n",
    "        \n",
    "    test_input1 = torch.FloatTensor(tmp_test[0]).to(device)\n",
    "    test_input2 = torch.LongTensor(tmp_test[1]).to(device)\n",
    "    test_input3 = torch.LongTensor(tmp_test[2]).to(device)\n",
    "    label = tmp_test[3]\n",
    "    \n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        pred = model(test_input1, test_input2, test_input3)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "    ql_09.append(np.maximum(0.9 * (label - pred[..., 4]), (1-0.9)*(pred[..., 4] - label)).mean() * 1000)\n",
    "    # ql_07.append(np.maximum(0.7 * (label - pred[..., 3]), (1-0.7)*(pred[..., 3] - label)).mean() * 1000)\n",
    "    ql_05.append(np.maximum(0.5 * (label - pred[..., 2]), (1-0.5)*(pred[..., 2] - label)).mean() * 1000)\n",
    "    # ql_03.append(np.maximum(0.3 * (label - pred[..., 1]), (1-0.3)*(pred[..., 1] - label)).mean() * 1000)\n",
    "    ql_01.append(np.maximum(0.1 * (label - pred[..., 0]), (1-0.1)*(pred[..., 0] - label)).mean() * 1000)\n",
    "        \n",
    "    qr_09.append((np.mean(label < pred[..., 4]), 0.9 - np.mean(label < pred[..., 4])))\n",
    "    # qr_07.append((np.mean(label < pred[..., 3]), 0.7 - np.mean(label < pred[..., 3])))\n",
    "    qr_05.append((np.mean(label < pred[..., 2]), 0.5 - np.mean(label < pred[..., 2])))\n",
    "    # qr_03.append((np.mean(label < pred[..., 1]), 0.3 - np.mean(label < pred[..., 1])))\n",
    "    qr_01.append((np.mean(label < pred[..., 0]), 0.1 - np.mean(label < pred[..., 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_hong",
   "language": "python",
   "name": "test_hong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
