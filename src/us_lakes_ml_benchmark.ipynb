{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.simplefilter('ignore', ValueWarning)\n",
    "\n",
    "os.chdir(os.path.abspath(''))\n",
    "df_mead = pd.read_csv(\"../data/us_lakes/df_mead_preprocessed.csv\")\n",
    "df_mohave = pd.read_csv(\"../data/us_lakes/df_mohave_preprocessed.csv\")\n",
    "df_havasu = pd.read_csv(\"../data/us_lakes/df_havasu_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(df_mead, df_mohave, df_havasu, valid_size=2/9, test_size=1/3):\n",
    "    \n",
    "    N, P = df_mohave.shape\n",
    "    \n",
    "    df_mead.columns = [\"e1\", \"i1\", \"o1\", \"y\", \"m\", \"d\"]\n",
    "    df_mohave.columns = [\"e2\", \"i2\", \"o2\", \"y\", \"m\", \"d\", \"p1\"]\n",
    "    df_havasu.columns = [\"e3\", \"i3\", \"o3\", \"y\", \"m\", \"d\", \"p2\"]\n",
    "        \n",
    "    index_1 = round(N * (1 - valid_size - test_size))\n",
    "    index_2 = round(N * (1-test_size))\n",
    "    \n",
    "    df_mead_train = df_mead.iloc[:index_1, :]\n",
    "    df_mohave_train = df_mohave.iloc[:index_1, :]\n",
    "    df_havasu_train = df_havasu.iloc[:index_1, :]\n",
    "    \n",
    "    df_mead_valid = df_mead.iloc[index_1:index_2, :]\n",
    "    df_mohave_valid = df_mohave.iloc[index_1:index_2, :]\n",
    "    df_havasu_valid = df_havasu.iloc[index_1:index_2, :]\n",
    "    \n",
    "    df_mead_test = df_mead.iloc[index_2:, :]\n",
    "    df_mohave_test = df_mohave.iloc[index_2:, :]\n",
    "    df_havasu_test = df_havasu.iloc[index_2:, :]\n",
    "    \n",
    "    return (df_mead_train, df_mohave_train, df_havasu_train), (df_mead_valid, df_mohave_valid, df_havasu_valid), (df_mead_test, df_mohave_test, df_havasu_test)\n",
    "\n",
    "train, valid, test = train_valid_test_split(df_mead.loc[df_mead[\"year\"] <= 2013],\n",
    "                                            df_mohave.loc[df_mohave[\"year\"] <= 2013],\n",
    "                                            df_havasu.loc[df_havasu[\"year\"] <= 2013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.arima import ARIMA, AutoARIMA\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "\n",
    "def generate_eval_arima(label_df, input_seq_len=24, tau=4):\n",
    "    col_labels = \"e3\" # ['wl_1018662', 'wl_1018680', 'wl_1018683', 'wl_1019630']\n",
    "    \n",
    "    tmp_df = np.array(label_df.loc[:, col_labels])\n",
    "    \n",
    "    n = tmp_df.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    conti_input = np.zeros((n, input_seq_len), dtype=np.float32)\n",
    "    label = np.zeros((n, tau))\n",
    "\n",
    "    for j in range(n):\n",
    "        conti_input[j, :] = tmp_df[j:(j+input_seq_len)]\n",
    "        label[j, :] = tmp_df[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "\n",
    "    return conti_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_range = [(2005, 2013), (2008, 2016), (2011, 2019), (2014, 2022)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test = train_valid_test_split(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "     \n",
    "    fh = ForecastingHorizon(np.arange(1, 5), is_relative=True)\n",
    "\n",
    "    ets_results = []\n",
    "    ets_forecaster = AutoETS(auto=True, n_jobs=-1)  \n",
    "\n",
    "    ets_forecaster.fit(tmp_train[2][\"e3\"])\n",
    "    \n",
    "    y_test_input, label = generate_eval_arima(tmp_test[2]) \n",
    "    \n",
    "    for i in range(y_test_input.shape[0]):\n",
    "        ets_forecaster.update(y_test_input[i], update_params=False)\n",
    "        tmp_result = ets_forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        ets_results.append(tmp_result.values[np.newaxis, ...])\n",
    "        \n",
    "    ets_results = np.concatenate(ets_results, axis=0)\n",
    "\n",
    "    ql_09.append(np.maximum(0.9 * (label - ets_results[..., 4]), (1-0.9)*(ets_results[..., 4] - label)).mean())\n",
    "    ql_07.append(np.maximum(0.7 * (label - ets_results[..., 3]), (1-0.7)*(ets_results[..., 3] - label)).mean())\n",
    "    ql_05.append(np.maximum(0.5 * (label - ets_results[..., 2]), (1-0.5)*(ets_results[..., 2] - label)).mean())\n",
    "    ql_03.append(np.maximum(0.3 * (label - ets_results[..., 1]), (1-0.3)*(ets_results[..., 1] - label)).mean())\n",
    "    ql_01.append(np.maximum(0.1 * (label - ets_results[..., 0]), (1-0.1)*(ets_results[..., 0] - label)).mean())\n",
    "        \n",
    "    qr_09.append((np.mean(label < ets_results[..., 4]), 0.9 - np.mean(label < ets_results[..., 4])))\n",
    "    qr_07.append((np.mean(label < ets_results[..., 3]), 0.7 - np.mean(label < ets_results[..., 3])))\n",
    "    qr_05.append((np.mean(label < ets_results[..., 2]), 0.5 - np.mean(label < ets_results[..., 2])))\n",
    "    qr_03.append((np.mean(label < ets_results[..., 1]), 0.3 - np.mean(label < ets_results[..., 1])))\n",
    "    qr_01.append((np.mean(label < ets_results[..., 0]), 0.1 - np.mean(label < ets_results[..., 0])))\n",
    "\n",
    "np.array(ql_09).mean().round(3)\n",
    "np.array(ql_05).mean().round(3)\n",
    "np.array(ql_01).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_09]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_09]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_05]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_05]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_01]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_01]).mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test = train_valid_test_split(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "     \n",
    "    fh = ForecastingHorizon(np.arange(1, 5), is_relative=True)\n",
    "\n",
    "    tmp_results = []\n",
    "    forecaster = AutoARIMA()  \n",
    "\n",
    "    forecaster.fit(tmp_train[2][\"e3\"])\n",
    "    \n",
    "    y_test_input, label = generate_eval_arima(tmp_test[2]) \n",
    "    \n",
    "    for i in range(y_test_input.shape[0]):\n",
    "        forecaster.update(y_test_input[i], update_params=False)\n",
    "        tmp_result = forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        tmp_results.append(tmp_result.values[np.newaxis, ...])\n",
    "        \n",
    "    tmp_results = np.concatenate(tmp_results, axis=0)\n",
    "    \n",
    "    ql_09.append(np.maximum(0.9 * (label - tmp_results[..., 4]), (1-0.9)*(tmp_results[..., 4] - label)).mean())\n",
    "    ql_07.append(np.maximum(0.7 * (label - tmp_results[..., 3]), (1-0.7)*(tmp_results[..., 3] - label)).mean())\n",
    "    ql_05.append(np.maximum(0.5 * (label - tmp_results[..., 2]), (1-0.5)*(tmp_results[..., 2] - label)).mean())\n",
    "    ql_03.append(np.maximum(0.3 * (label - tmp_results[..., 1]), (1-0.3)*(tmp_results[..., 1] - label)).mean())\n",
    "    ql_01.append(np.maximum(0.1 * (label - tmp_results[..., 0]), (1-0.1)*(tmp_results[..., 0] - label)).mean())\n",
    "        \n",
    "    qr_09.append((np.mean(label < tmp_results[..., 4]), 0.9 - np.mean(label < tmp_results[..., 4])))\n",
    "    qr_07.append((np.mean(label < tmp_results[..., 3]), 0.7 - np.mean(label < tmp_results[..., 3])))\n",
    "    qr_05.append((np.mean(label < tmp_results[..., 2]), 0.5 - np.mean(label < tmp_results[..., 2])))\n",
    "    qr_03.append((np.mean(label < tmp_results[..., 1]), 0.3 - np.mean(label < tmp_results[..., 1])))\n",
    "    qr_01.append((np.mean(label < tmp_results[..., 0]), 0.1 - np.mean(label < tmp_results[..., 0])))\n",
    "\n",
    "np.array(ql_09).mean().round(3)\n",
    "np.array(ql_05).mean().round(3)\n",
    "np.array(ql_01).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_09]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_09]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_05]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_05]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_01]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_01]).mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test = train_valid_test_split(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "     \n",
    "    fh = ForecastingHorizon(np.arange(1, 5), is_relative=True)\n",
    "\n",
    "    tmp_results = []\n",
    "    forecaster = ThetaForecaster()  \n",
    "\n",
    "    forecaster.fit(tmp_train[2][\"e3\"])\n",
    "    \n",
    "    y_test_input, label = generate_eval_arima(tmp_test[2]) \n",
    "    \n",
    "    for i in range(y_test_input.shape[0]):\n",
    "        forecaster.update(y_test_input[i], update_params=False)\n",
    "        tmp_result = forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        tmp_results.append(tmp_result.values[np.newaxis, ...])\n",
    "        \n",
    "    tmp_results = np.concatenate(tmp_results, axis=0)\n",
    "    \n",
    "    ql_09.append(np.maximum(0.9 * (label - tmp_results[..., 4]), (1-0.9)*(tmp_results[..., 4] - label)).mean())\n",
    "    ql_07.append(np.maximum(0.7 * (label - tmp_results[..., 3]), (1-0.7)*(tmp_results[..., 3] - label)).mean())\n",
    "    ql_05.append(np.maximum(0.5 * (label - tmp_results[..., 2]), (1-0.5)*(tmp_results[..., 2] - label)).mean())\n",
    "    ql_03.append(np.maximum(0.3 * (label - tmp_results[..., 1]), (1-0.3)*(tmp_results[..., 1] - label)).mean())\n",
    "    ql_01.append(np.maximum(0.1 * (label - tmp_results[..., 0]), (1-0.1)*(tmp_results[..., 0] - label)).mean())\n",
    "        \n",
    "    qr_09.append((np.mean(label < tmp_results[..., 4]), 0.9 - np.mean(label < tmp_results[..., 4])))\n",
    "    qr_07.append((np.mean(label < tmp_results[..., 3]), 0.7 - np.mean(label < tmp_results[..., 3])))\n",
    "    qr_05.append((np.mean(label < tmp_results[..., 2]), 0.5 - np.mean(label < tmp_results[..., 2])))\n",
    "    qr_03.append((np.mean(label < tmp_results[..., 1]), 0.3 - np.mean(label < tmp_results[..., 1])))\n",
    "    qr_01.append((np.mean(label < tmp_results[..., 0]), 0.1 - np.mean(label < tmp_results[..., 0])))\n",
    "\n",
    "\n",
    "np.array(ql_09).mean().round(3)\n",
    "np.array(ql_05).mean().round(3)\n",
    "np.array(ql_01).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_09]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_09]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_05]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_05]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_01]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_01]).mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb \n",
    "from sktime.transformations.series.fourier import FourierFeatures\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "def generate_data_for_lgb(df_lakes, term_list, scaler=None, input_seq_len=24, tau=4):\n",
    "    col_labels = \"e3\" \n",
    "    \n",
    "\n",
    "    tmp_arr = np.array(df_lakes.drop(columns=['y', 'm', 'd']))\n",
    "    \n",
    "    tmp_cate = np.array(df_lakes[[\"y\", \"m\", \"d\"]])\n",
    "    \n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        tmp_arr = scaler.fit_transform(tmp_arr)\n",
    "\n",
    "    if scaler:\n",
    "        tmp_arr = scaler.fit_transform(tmp_arr)\n",
    "        \n",
    "    tmp_label = np.array(df_lakes.loc[:, col_labels])\n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    input = np.zeros((n, input_seq_len, p + term_list * 2), dtype=np.float32)\n",
    "    cate_input = np.zeros((n, input_seq_len, 3), dtype=np.int32)\n",
    "    label = np.zeros((n, tau))\n",
    "    \n",
    "    transformer = FourierFeatures(sp_list=[30], fourier_terms_list=[term_list])\n",
    "    \n",
    "    for j in range(n):\n",
    "        input[j, :] = np.concatenate([tmp_arr[j:(j+input_seq_len)], transformer.fit_transform(tmp_label[j:(j+input_seq_len)])], axis=1)\n",
    "        cate_input[j, :] = tmp_cate[j:(j+input_seq_len)]\n",
    "        label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "\n",
    "    return input.reshape(n, input_seq_len * (p + term_list * 2)), cate_input.reshape(n, -1), label, scaler \n",
    "\n",
    "\n",
    "\n",
    "term_list = 2\n",
    "\n",
    "data_split_range = [(2005, 2013), (2008, 2016), (2011, 2019), (2014, 2022)]\n",
    "\n",
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test = train_valid_test_split(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "    \n",
    "    train_df_lakes = pd.concat([tmp_train[0].drop(columns=['y', 'm', 'd']), tmp_train[1].drop(columns=['y', 'm', 'd']), tmp_train[2]], axis=1)\n",
    "    \n",
    "    valid_df_lakes = pd.concat([tmp_valid[0].drop(columns=['y', 'm', 'd']), tmp_valid[1].drop(columns=['y', 'm', 'd']), tmp_valid[2]], axis=1)\n",
    "    \n",
    "    train_input_lgb, train_cate_lgb, train_label_lgb, scaler = generate_data_for_lgb(train_df_lakes, term_list=term_list)\n",
    "    \n",
    "    valid_input_lgb, valid_cate_lgb, valid_label_lgb, _ = generate_data_for_lgb(valid_df_lakes, scaler=scaler, term_list=term_list)\n",
    "    \n",
    "    test_df_lakes = pd.concat([tmp_test[0].drop(columns=['y', 'm', 'd']), tmp_test[1].drop(columns=['y', 'm', 'd']), tmp_test[2]], axis=1)\n",
    "    \n",
    "    test_input_lgb, test_cate_lgb, test_label_lgb, _ = generate_data_for_lgb(test_df_lakes, term_list=term_list, scaler=scaler)\n",
    "    \n",
    "    alphas = [0.1, 0.5, 0.9]\n",
    "\n",
    "    model_list = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        params = {\n",
    "            \"objective\": \"quantile\",\n",
    "            \"alpha\": alpha,\n",
    "            \"num_leaves\": 20, \n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"categorical_feature\": np.arange(312, 384),\n",
    "            \"eval_set\": [np.concatenate([valid_input_lgb, valid_cate_lgb], axis=1), valid_label_lgb],\n",
    "            \"eval_metric\": \"quantile\",\n",
    "            \"verbose\": 0,\n",
    "            \"reg_alpha\": 1.,\n",
    "            \"reg_lambda\" : 1.,\n",
    "            \"n_estimators\": 5\n",
    "        }\n",
    "        gbm = lgb.LGBMRegressor(**params)\n",
    "        regr_multiglb = MultiOutputRegressor(gbm)\n",
    "        regr_multiglb.fit(np.concatenate([train_input_lgb, train_cate_lgb], axis=1), train_label_lgb)\n",
    "        \n",
    "        model_list.append(\n",
    "            regr_multiglb\n",
    "        )\n",
    "    \n",
    "    lgb_results_01 = model_list[0].predict(np.concatenate([test_input_lgb, test_cate_lgb], axis=1))\n",
    "    #lgb_results_03 = model_list[1].predict(test_input_lgb)\n",
    "    lgb_results_05 = model_list[1].predict(np.concatenate([test_input_lgb, test_cate_lgb], axis=1))\n",
    "    #lgb_results_07 = model_list[3].predict(test_input_lgb)\n",
    "    lgb_results_09 = model_list[2].predict(np.concatenate([test_input_lgb, test_cate_lgb], axis=1))\n",
    "\n",
    "    \n",
    "    ql_09.append(np.maximum(0.9 * (test_label_lgb - lgb_results_09), (1-0.9)*(lgb_results_09 - test_label_lgb)).mean())\n",
    "    #ql_07.append(np.maximum(0.7 * (test_label_lgb - lgb_results_07), (1-0.7)*(lgb_results_07 - test_label_lgb)).mean())\n",
    "    ql_05.append(np.maximum(0.5 * (test_label_lgb - lgb_results_05), (1-0.5)*(lgb_results_05 - test_label_lgb)).mean())\n",
    "    #ql_03.append(np.maximum(0.3 * (test_label_lgb - lgb_results_03), (1-0.3)*(lgb_results_03 - test_label_lgb)).mean())\n",
    "    ql_01.append(np.maximum(0.1 * (test_label_lgb - lgb_results_01), (1-0.1)*(lgb_results_01 - test_label_lgb)).mean())\n",
    "        \n",
    "    qr_09.append((np.mean(test_label_lgb < lgb_results_09), 0.9 - np.mean(test_label_lgb < lgb_results_09)))\n",
    "    #qr_07.append((np.mean(test_label_lgb < lgb_results_07), 0.7 - np.mean(test_label_lgb < lgb_results_07)))\n",
    "    qr_05.append((np.mean(test_label_lgb < lgb_results_05), 0.5 - np.mean(test_label_lgb < lgb_results_05)))\n",
    "    #qr_03.append((np.mean(test_label_lgb < lgb_results_03), 0.3 - np.mean(test_label_lgb < lgb_results_03)))\n",
    "    qr_01.append((np.mean(test_label_lgb < lgb_results_01), 0.1 - np.mean(test_label_lgb < lgb_results_01)))\n",
    "\n",
    "np.array(ql_09).mean().round(3)\n",
    "np.array(ql_05).mean().round(3)\n",
    "np.array(ql_01).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_09]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_09]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_05]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_05]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_01]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_01]).mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_hong",
   "language": "python",
   "name": "test_hong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
