{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = sorted(glob(\"../data/raw/water_data/*.csv\"))\n",
    "print(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in w_list[4:-1]:\n",
    "    \n",
    "    tmp = pd.read_csv(i)\n",
    "    tmp = tmp.replace(\" \", np.nan)\n",
    "    tmp.drop('fw_1018680', axis=1, inplace=True)\n",
    "    tmp['ymdhm'] = pd.to_datetime(tmp['ymdhm'], infer_datetime_format=True, errors='ignore')\n",
    "    tmp['swl'] = tmp['swl'].replace(0, np.NaN)\n",
    "    tmp['sfw'] = tmp['sfw'].replace(-0.01, np.NaN)\n",
    "    tmp['sfw'] = tmp['sfw'].replace(0, np.NaN)\n",
    "    tmp['ecpc'].loc[tmp['ecpc'] > 200] = np.NaN\n",
    "    tmp['tototf'].loc[tmp['tototf'] < 0] = np.NaN\n",
    "    tmp['tototf'].loc[tmp['tototf'] > 20000] = np.NaN\n",
    "    \n",
    "    tmp_grouped = tmp.groupby([tmp['ymdhm'].dt.year, tmp['ymdhm'].dt.month, tmp['ymdhm'].dt.day, tmp['ymdhm'].dt.hour]).mean()\n",
    "    tmp_grouped.interpolate(method='linear', axis=0, inplace=True)\n",
    "\n",
    "    tmp_grouped[\"year\"] = tmp_grouped.index.get_level_values(0)\n",
    "    tmp_grouped[\"month\"] = tmp_grouped.index.get_level_values(1)\n",
    "    tmp_grouped[\"day\"] = tmp_grouped.index.get_level_values(2)\n",
    "    tmp_grouped[\"hour\"] = tmp_grouped.index.get_level_values(3)\n",
    "\n",
    "    tmp_grouped.reset_index(drop=True)\n",
    "    \n",
    "    data.append(tmp_grouped)\n",
    "    \n",
    "    \n",
    "df_hourly_hanriver = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly_hanriver = df_hourly_hanriver.loc[:, ['tide_level', 'swl', 'inf', 'sfw', 'ecpc', 'tototf', 'wl_1018662', 'fw_1018662','wl_1018680', 'wl_1018683', 'fw_1018683', 'wl_1019630', 'fw_1019630', 'year', 'month', 'day', 'hour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rf = '../data/raw/rf_data/'\n",
    "\n",
    "rf_list = os.listdir(path_rf)\n",
    "rf_list_py = [file for file in rf_list if file.endswith('.csv')]\n",
    "\n",
    "rain_df = pd.DataFrame()\n",
    "for i in sorted(rf_list_py)[4:-1]:\n",
    "    data2 = pd.read_csv(path_rf + i)\n",
    "    rain_df = pd.concat([rain_df,data2])\n",
    "    \n",
    "rain_df = rain_df.reset_index(drop = True)\n",
    "\n",
    "rain_df['ymdhm'] = pd.to_datetime(rain_df['ymdhm'], infer_datetime_format=True, errors='ignore')\n",
    "rain_grouped = rain_df.groupby([rain_df['ymdhm'].dt.year, rain_df['ymdhm'].dt.month, rain_df['ymdhm'].dt.day, rain_df['ymdhm'].dt.hour]).mean()\n",
    "rain_grouped.interpolate(method='linear', axis=0, inplace=True)\n",
    "rain_grouped[\"year\"] = rain_grouped.index.get_level_values(0)\n",
    "rain_grouped[\"month\"] = rain_grouped.index.get_level_values(1)\n",
    "rain_grouped[\"day\"] = rain_grouped.index.get_level_values(2)\n",
    "rain_grouped[\"hour\"] = rain_grouped.index.get_level_values(3)\n",
    "rain_grouped.reset_index(drop=True, inplace=True)\n",
    "rain_grouped = rain_grouped.loc[:, ['year', 'month', 'day', 'hour', 'rf_10184100', 'rf_10184110', 'rf_10184140']]\n",
    "rain_grouped.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = rain_grouped.merge(df_hourly_hanriver, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "df_merged.to_csv(\"../data/df_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"month\"] = df_merged[\"month\"] - 5\n",
    "df_merged[\"day\"] = df_merged[\"day\"] - 1\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "### min-max \n",
    "fitted = min_max_scaler.fit(df_merged.iloc[:, 4:])\n",
    "array_scaled_merged = min_max_scaler.transform(df_merged.iloc[:, 4:])\n",
    "\n",
    "train_scaled_array = array_scaled_merged[df_merged['year'] != 2021, :]\n",
    "df_train_total = pd.concat([df_merged.loc[df_merged['year'] != 2021, ['year', 'month', 'day', 'hour']], pd.DataFrame(train_scaled_array)], axis=1)\n",
    "\n",
    "df_train_total.to_csv(\"../data/df_train_total.csv\", index=False)\n",
    "\n",
    "test_scaled_array = array_scaled_merged[df_merged['year'] == 2021, :]\n",
    "df_test_total = pd.concat([df_merged.loc[df_merged['year'] == 2021, ['year', 'month', 'day', 'hour']].reset_index(drop=True), pd.DataFrame(test_scaled_array)], axis=1)\n",
    "\n",
    "df_test_total.to_csv(\"../data/df_test_total.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution-shift case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in [2016, 2017, 2018, 2019, 2020, 2021]:\n",
    "    tmp_merged = df_merged.loc[df_merged[\"year\"] == y, ]\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    tmp_merged.loc[tmp_merged[\"month\"].isin([0, 1])]\n",
    "    ### min-max \n",
    "    fitted = min_max_scaler.fit(tmp_merged.loc[tmp_merged[\"month\"].isin([0, 1])].iloc[:, 4:])\n",
    "    array_scaled_merged_ds = min_max_scaler.transform(tmp_merged.iloc[:, 4:])\n",
    "\n",
    "    train_scaled_array = array_scaled_merged_ds[tmp_merged[\"month\"].isin([0, 1]), :]\n",
    "    df_train_total = pd.concat([tmp_merged.loc[tmp_merged[\"month\"].isin([0, 1]), ['year', 'month', 'day', 'hour']].reset_index(drop=True), pd.DataFrame(train_scaled_array)], axis=1)\n",
    "\n",
    "    df_train_total.to_csv(\"../data/df_train_total_ds_{}.csv\".format(str(y)), index=False)\n",
    "\n",
    "    test_scaled_array = array_scaled_merged_ds[tmp_merged[\"month\"].isin([2, 3]), :]\n",
    "    df_test_total = pd.concat([tmp_merged.loc[tmp_merged[\"month\"].isin([2, 3]), ['year', 'month', 'day', 'hour']].reset_index(drop=True), pd.DataFrame(test_scaled_array)], axis=1)\n",
    "\n",
    "    df_test_total.to_csv(\"../data/df_test_total_ds_{}.csv\".format(str(y)), index=False)\n",
    "    \n",
    "    tmp_merged.loc[tmp_merged['month'].isin([0,1,2,3]), :].to_csv(\"../data/df_merged_ds_{}.csv\".format(str(y)), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
