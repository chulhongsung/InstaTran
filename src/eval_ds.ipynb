{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.abspath(''))\n",
    "sys.path.append(os.path.abspath(os.path.abspath('')))\n",
    "sys.path.append(os.path.abspath(os.path.abspath('') + '/src'))\n",
    "\n",
    "from utils import *\n",
    "from models import *\n",
    "from utils import *\n",
    "from layers import *\n",
    "from models import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from ing_theme_matplotlib import mpl_style\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from models import *\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 60\n",
    "mpl_style(dark=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ding = STALSTM(48, 16, 12, 5)\n",
    "\n",
    "adj = torch.tensor([[1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "                    [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]]\n",
    "        ).float()\n",
    "\n",
    "norm_adj = adj/adj.sum(dim=-1).unsqueeze(-1)\n",
    "deng = HSDSTM(adj=norm_adj,                  \n",
    "                input_size=16,\n",
    "                seq_len=48,\n",
    "                num_channels=[16, 16],\n",
    "                node_dim=1,\n",
    "                dropout=0.1,\n",
    "                num_levels=3,\n",
    "                tau=12,\n",
    "                num_quantiles=5)\n",
    "\n",
    "\n",
    "deepar = DeepAR(\n",
    "        d_input=16, \n",
    "        d_embedding=3, \n",
    "        n_embedding=[16, 32, 24], \n",
    "        d_model=30, \n",
    "        num_targets=1, \n",
    "        n_layers=3,\n",
    "        dr=0.1\n",
    "    )\n",
    "\n",
    "mqrnn = MQRnn(\n",
    "        d_input=16,\n",
    "        d_embedding=1,\n",
    "        n_embedding=[16, 32, 24],\n",
    "        d_model=5,\n",
    "        tau=12,\n",
    "        num_targets=1,\n",
    "        num_quantiles=5,\n",
    "        n_layers=3,\n",
    "        dr=0.1\n",
    "    )\n",
    "\n",
    "quanilte_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "tft = TemporalFusionTransformer(\n",
    "    d_model=30,\n",
    "    d_embedding=5,\n",
    "    cate_dims=[16, 32, 24],\n",
    "    num_cv=16,\n",
    "    seq_len=48,\n",
    "    num_targets=1,\n",
    "    tau=12,\n",
    "    quantile=quanilte_levels,\n",
    "    dr=0.1,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "sps = torch.tensor([ [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n",
    "                    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
    "                ).float()\n",
    "\n",
    "instatran = InstaTran(\n",
    "    d_model=20,\n",
    "    d_embedding=3,\n",
    "    cate_dims=[16, 32, 24],\n",
    "    spatial_structure=sps,\n",
    "    num_cv=16,\n",
    "    seq_len=48,\n",
    "    num_targets=1,\n",
    "    tau=12,\n",
    "    quantile=quanilte_levels,\n",
    "    dr=0.1,\n",
    "    device=device\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ts_data_for_deng(df, label_df, input_seq_len=48, tau=12):\n",
    "    conti_input_list = []\n",
    "    label_list = []\n",
    "    col_labels =  ['wl_1018680'] # ['wl_1018662', 'wl_1018680', 'wl_1018683', 'wl_1019630']\n",
    "    \n",
    "    for i in df['year'].unique():\n",
    "        tmp_df = np.array(df.loc[df['year'] == i, :])\n",
    "        tmp_label_df = np.array(label_df.loc[label_df['year'] == i, col_labels])\n",
    "        n = tmp_df.shape[0] - input_seq_len - tau \n",
    "        \n",
    "        tmp_conti_input = tmp_df[:, 4:] # (4416, 16)\n",
    "        \n",
    "        conti_input = np.zeros((n, input_seq_len, tmp_conti_input.shape[1]), dtype=np.float32)\n",
    "        label = np.zeros((n, tau, len(col_labels)))\n",
    "    \n",
    "        for j in range(n):\n",
    "            conti_input[j, :, :] = tmp_conti_input[j:(j+input_seq_len), :]\n",
    "            label[j, :, :] = tmp_label_df[(j+input_seq_len):(j+input_seq_len+tau), :]/1000\n",
    "\n",
    "        conti_input_list.append(conti_input)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    total_conti_input = np.concatenate(conti_input_list, axis=0)\n",
    "    total_label = np.concatenate(label_list, axis=0)\n",
    "    \n",
    "    return np.swapaxes(total_conti_input, 2, 1), total_label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepar_05_ql_results, deepar_07_ql_results, deepar_09_ql_results, deepar_05_qr_results, deepar_07_qr_results, deepar_09_qr_results = [], [], [], [], [], []\n",
    "mqrnn_05_ql_results, mqrnn_07_ql_results, mqrnn_09_ql_results, mqrnn_05_qr_results, mqrnn_07_qr_results, mqrnn_09_qr_results = [], [], [], [], [], []\n",
    "tft_05_ql_results, tft_07_ql_results, tft_09_ql_results, tft_05_qr_results, tft_07_qr_results, tft_09_qr_results = [], [], [], [], [], []\n",
    "instatran_05_ql_results, instatran_07_ql_results, instatran_09_ql_results, instatran_05_qr_results, instatran_07_qr_results, instatran_09_qr_results = [], [], [], [], [], []\n",
    "ding_05_ql_results, ding_07_ql_results, ding_09_ql_results, ding_05_qr_results, ding_07_qr_results, ding_09_qr_results = [], [], [], [], [], []\n",
    "deng_05_ql_results, deng_07_ql_results, deng_09_ql_results, deng_05_qr_results, deng_07_qr_results, deng_09_qr_results = [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "    df_test_total = pd.read_csv(\"../data/df_test_total_ds_{}.csv\".format(year))\n",
    "    df_merged = pd.read_csv(\"../data/df_merged_ds_{}.csv\".format(year))\n",
    "\n",
    "\n",
    "    deepar.load_state_dict(torch.load('../assets/ds/ds_DeepAR_{}_best.pth'.format(year), map_location='cpu'))\n",
    "    mqrnn.load_state_dict(torch.load('../assets/ds/ds_MQRnn_{}_best.pth'.format(year), map_location='cpu'))\n",
    "    tft.load_state_dict(torch.load('../assets/ds/ds_TFT_{}_best.pth'.format(year), map_location='cpu'))\n",
    "    ding.load_state_dict(torch.load('../assets/ds/ds_STALSTM_{}_best.pth'.format(year), map_location='cpu'))\n",
    "    deng.load_state_dict(torch.load('../assets/ds/ds_HSDSTM_{}_best.pth'.format(year), map_location='cpu'))\n",
    "    instatran.load_state_dict(torch.load('../assets/ds/ds_InstaTran_{}_best.pth'.format(year), map_location='cpu'))\n",
    "\n",
    "    deepar.eval()\n",
    "    mqrnn.eval()\n",
    "    tft.eval()\n",
    "    ding.eval()\n",
    "    deng.eval()\n",
    "    instatran.eval()\n",
    "\n",
    "    test_conti, test_cate, test_future, eval_label = generate_ts_data(df_test_total, df_merged, input_seq_len=48, tau=12)\n",
    "    eval_label = torch.tensor(eval_label)\n",
    "\n",
    "    test_input_for_deng, _ = generate_ts_data_for_deng(df_test_total, df_merged)\n",
    "\n",
    "    deepar.eval()\n",
    "    deepar_output = deepar(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "    deepar_mu, deepar_sigma = deepar_output\n",
    "    deepar_output = gaussian_quantile(deepar_mu, deepar_sigma)\n",
    "    deepar_output = torch.tensor(deepar_output)\n",
    "\n",
    "    mqrnn.eval()\n",
    "    mqrnn_output = mqrnn(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "\n",
    "\n",
    "    tft.eval()\n",
    "    tft_output = tft(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "\n",
    "    instatran.eval()\n",
    "    instatran_output, ssa_weight1, ssa_weight2, tsa_weight, dec_weights, fi1, fi2 = instatran(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "\n",
    "    deng_output = deng(torch.tensor(test_input_for_deng))\n",
    "    ding_output, alpha, beta = ding(torch.tensor(test_conti))\n",
    "\n",
    "    deepar_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - deepar_output[..., 4].squeeze()), (1-0.9)*(deepar_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    deepar_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - deepar_output[..., 3].squeeze()), (1-0.7)*(deepar_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    deepar_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - deepar_output[..., 2].squeeze()), (1-0.5)*(deepar_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    deepar_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    deepar_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    deepar_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < deepar_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])\n",
    "\n",
    "    mqrnn_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - mqrnn_output[..., 4].squeeze()), (1-0.9)*(mqrnn_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    mqrnn_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - mqrnn_output[..., 3].squeeze()), (1-0.7)*(mqrnn_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    mqrnn_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - mqrnn_output[..., 2].squeeze()), (1-0.5)*(mqrnn_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    mqrnn_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    mqrnn_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    mqrnn_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < mqrnn_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])\n",
    "\n",
    "\n",
    "    tft_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - tft_output[..., 4].squeeze()), (1-0.9)*(tft_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    tft_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - tft_output[..., 3].squeeze()), (1-0.7)*(tft_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    tft_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - tft_output[..., 2].squeeze()), (1-0.5)*(tft_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    tft_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    tft_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    tft_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < tft_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])\n",
    "\n",
    "    instatran_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - instatran_output[..., 4].squeeze()), (1-0.9)*(instatran_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    instatran_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - instatran_output[..., 3].squeeze()), (1-0.7)*(instatran_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    instatran_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - instatran_output[..., 2].squeeze()), (1-0.5)*(instatran_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    instatran_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    instatran_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    instatran_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < instatran_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])\n",
    "\n",
    "    ding_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - ding_output[..., 4].squeeze()), (1-0.9)*(ding_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    ding_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - ding_output[..., 3].squeeze()), (1-0.7)*(ding_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    ding_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - ding_output[..., 2].squeeze()), (1-0.5)*(ding_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    ding_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    ding_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    ding_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < ding_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])\n",
    "\n",
    "    deng_09_ql_results.append(torch.maximum(0.9 * (eval_label.squeeze() - deng_output[..., 4].squeeze()), (1-0.9)*(deng_output[..., 4].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    deng_07_ql_results.append(torch.maximum(0.7 * (eval_label.squeeze() - deng_output[..., 3].squeeze()), (1-0.7)*(deng_output[..., 3].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "    deng_05_ql_results.append(torch.maximum(0.5 * (eval_label.squeeze() - deng_output[..., 2].squeeze()), (1-0.5)*(deng_output[..., 2].squeeze() -eval_label.squeeze() )).mean().detach().numpy().round(4))\n",
    "\n",
    "    deng_09_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 4].squeeze().detach().cpu().numpy()).round(4), np.abs(0.9 - np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 4].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    deng_07_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 3].squeeze().detach().cpu().numpy()).round(4), np.abs(0.7 - np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 3].squeeze().detach().cpu().numpy())).round(4)])\n",
    "    deng_05_qr_results.append([np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 2].squeeze().detach().cpu().numpy()).round(4), np.abs(0.5 - np.mean(eval_label.squeeze().cpu().numpy() < deng_output[..., 2].squeeze().detach().cpu().numpy())).round(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(deepar_09_ql_results).round(4),\n",
    "    np.mean(deepar_07_ql_results).round(4),\n",
    "    np.mean(deepar_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(deepar_09_qr_results, axis=0).round(4),\n",
    "    np.mean(deepar_07_qr_results, axis=0).round(4),\n",
    "    np.mean(deepar_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MQRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(mqrnn_09_ql_results).round(4),\n",
    "    np.mean(mqrnn_07_ql_results).round(4),\n",
    "    np.mean(mqrnn_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(mqrnn_09_qr_results, axis=0).round(4),\n",
    "    np.mean(mqrnn_07_qr_results, axis=0).round(4),\n",
    "    np.mean(mqrnn_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(tft_09_ql_results).round(4),\n",
    "    np.mean(tft_07_ql_results).round(4),\n",
    "    np.mean(tft_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(tft_09_qr_results, axis=0).round(4),\n",
    "    np.mean(tft_07_qr_results, axis=0).round(4),\n",
    "    np.mean(tft_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InstaTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(instatran_09_ql_results).round(4),\n",
    "    np.mean(instatran_07_ql_results).round(4),\n",
    "    np.mean(instatran_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(instatran_09_qr_results, axis=0).round(4),\n",
    "    np.mean(instatran_07_qr_results, axis=0).round(4),\n",
    "    np.mean(instatran_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STA-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(ding_09_ql_results).round(4),\n",
    "    np.mean(ding_07_ql_results).round(4),\n",
    "    np.mean(ding_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(ding_09_qr_results, axis=0).round(4),\n",
    "    np.mean(ding_07_qr_results, axis=0).round(4),\n",
    "    np.mean(ding_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HSDSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(deng_09_ql_results).round(4),\n",
    "    np.mean(deng_07_ql_results).round(4),\n",
    "    np.mean(deng_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(deng_09_qr_results, axis=0).round(4),\n",
    "    np.mean(deng_07_qr_results, axis=0).round(4),\n",
    "    np.mean(deng_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_total = pd.read_csv(\"../data/df_test_total_ds_2017.csv\")\n",
    "df_merged = pd.read_csv(\"../data/df_merged_ds_2017.csv\")\n",
    "\n",
    "tft.load_state_dict(torch.load('../assets/ds/ds_TFT_2017_best.pth', map_location='cpu'))\n",
    "instatran.load_state_dict(torch.load('../assets/ds/ds_InstaTran_2017_best.pth', map_location='cpu'))\n",
    "\n",
    "\n",
    "instatran.eval()\n",
    "stt_output, ssa_weight1, ssa_weight2, tsa_weight, dec_weights, fi1, fi2 = instatran(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "\n",
    "test_conti, test_cate, test_future, eval_label = generate_ts_data(df_test_total, df_merged, input_seq_len=48, tau=12)\n",
    "\n",
    "tft.eval()\n",
    "confe_output = tft.confe(torch.FloatTensor(test_conti)) \n",
    "catfe_output = tft.catfe(torch.LongTensor(test_cate))  \n",
    "obs_feature = torch.cat([confe_output, catfe_output], axis=-2) \n",
    "x1, tft_vsn_output  = tft.vsn1(obs_feature) \n",
    "\n",
    "tft_output = tft(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 1\n",
    "f, (ax1, ax2) = plt.subplots(ncols=1, nrows=2, sharex=True)\n",
    "\n",
    "sns.lineplot(test_conti[::48, :, feature_idx].squeeze().reshape(-1)[30:], label=r\"Observation of $P_1$\", ax=ax1).set(xlabel=\"Time points\")\n",
    "sns.lineplot(test_conti[::48, :, feature_idx].squeeze().reshape(-1)[30:], label=r\"Observation of $P_1$\", ax=ax2)\n",
    "sns.lineplot(fi1.detach().cpu()[::48, :, feature_idx, 0].reshape(-1)[30:], linestyle='--', label=r\"Importance of $P_1$ (InstaTran)\", ax=ax2)\n",
    "sns.lineplot(tft_vsn_output.detach().cpu()[::48, :, feature_idx, 0].reshape(-1)[30:], linestyle=':', label=r\"Importance of $P_1$ (TFT)\", ax=ax2)\n",
    "\n",
    "ax1.set_ylim(0.5, 3)\n",
    "ax2.set_ylim(0, 0.2)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "\n",
    "ax1.set_ylabel(\"\")\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_xlabel(\"Time points\")\n",
    "\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "\n",
    "ax1.get_legend().remove()\n",
    "ax2.get_legend().remove()\n",
    "\n",
    "d = .7    \n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=15, linestyle=\"none\", color='k', clip_on=False)\n",
    "\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax1.set_yticks([0.5, 1.0, 2.0, 3.0])\n",
    "ax1.set_yticklabels(['0.5', '1.0', '2.0', '3.0'], fontsize = 12)\n",
    "\n",
    "ax2.set_yticks([0.0, 0.1, 0.2])\n",
    "ax2.set_yticklabels(['0.0', '0.1', '0.2'], fontsize = 12)\n",
    "\n",
    "ax2.set_xticks([100 * i + 2 for i in range(15)])\n",
    "ax2.set_xticklabels([str(100 * i) for i in range(15)] ,fontsize = 10)\n",
    "ax2.xaxis.grid()\n",
    "\n",
    "ax1.set_xlim(0, 402)\n",
    "ax2.set_xlim(0, 402)\n",
    "h,l = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handles=h, fontsize=\"12\", loc='upper right', ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_total = pd.read_csv(\"../data/df_test_total_ds_2017.csv\")\n",
    "df_merged = pd.read_csv(\"../data/df_merged_ds_2017.csv\")\n",
    "\n",
    "tft.load_state_dict(torch.load('../assets/ds/ds_TFT_2017_best.pth', map_location='cpu'))\n",
    "instatran.load_state_dict(torch.load('../assets/ds/ds_InstaTran_2017_best.pth', map_location='cpu'))\n",
    "\n",
    "\n",
    "instatran.eval()\n",
    "stt_output, ssa_weight1, ssa_weight2, tsa_weight, dec_weights, fi1, fi2 = instatran(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))\n",
    "\n",
    "test_conti, test_cate, test_future, eval_label = generate_ts_data(df_test_total, df_merged, input_seq_len=48, tau=12)\n",
    "\n",
    "tft.eval()\n",
    "confe_output = tft.confe(torch.FloatTensor(test_conti)) \n",
    "catfe_output = tft.catfe(torch.LongTensor(test_cate))  \n",
    "obs_feature = torch.cat([confe_output, catfe_output], axis=-2) \n",
    "x1, tft_vsn_output  = tft.vsn1(obs_feature) \n",
    "\n",
    "tft_output = tft(torch.FloatTensor(test_conti), torch.LongTensor(test_cate), torch.LongTensor(test_future))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          \n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     \n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)  \n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)   \n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)   \n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)  \n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  \n",
    "\n",
    "B, tau, num_targets, quantiles = stt_output.shape\n",
    "\n",
    "true = eval_label\n",
    "\n",
    "if type(stt_output) == torch.Tensor:    \n",
    "    preds = stt_output.detach().cpu().numpy()\n",
    "\n",
    "preds_ = preds[::tau, ...].reshape(-1, num_targets, quantiles)\n",
    "true_ = true[::tau, ...].reshape(-1, num_targets)\n",
    "instatran_results = preds_[:, 0, :]\n",
    "\n",
    "df_instatran_results = pd.DataFrame({\"10%\": instatran_results[:, 0],\n",
    "                            \"90%\": instatran_results[:, 4],\n",
    "                            \"Target\": true_[:, 0]}).reset_index().melt(id_vars=['index'])\n",
    "\n",
    "palette = {\n",
    "    'Target': 'white' if False else 'black'\n",
    "}\n",
    "\n",
    "forecasting_position = [tau * x for x in range(B // tau + 1)]\n",
    "\n",
    "B, tau, num_targets, quantiles = tft_output.shape\n",
    "\n",
    "true = eval_label\n",
    "\n",
    "if type(tft_output) == torch.Tensor:    \n",
    "    preds2 = tft_output.detach().cpu().numpy()\n",
    "\n",
    "preds2_ = preds2[::tau, ...].reshape(-1, num_targets, quantiles)\n",
    "tft_result = preds2_[:, 0, :]\n",
    "true_.shape[0]\n",
    "df_tft_result = pd.DataFrame({\"10%\": tft_result[:, 0],\n",
    "                            \"90%\": tft_result[:, 4],\n",
    "                            \"Target\": tft_result[:, 0]}).reset_index().melt(id_vars=['index'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))   \n",
    "ax1 = sns.lineplot(x='index', y='value', hue='variable', data=df_instatran_results.loc[df_instatran_results['variable'] == 'Target',], palette=palette)\n",
    "conf = ax.fill_between(np.arange(preds_.shape[0]), df_instatran_results.loc[df_instatran_results['variable'] == '10%', 'value'], df_instatran_results.loc[df_instatran_results['variable'] == '90%', 'value'], color='blue', alpha=0.3, label=r'80% interval (InstaTran )')\n",
    "ax1.set(xlim=(0, true_.shape[0]), ylabel='Water Level/1000')\n",
    "ax1.set_xlabel(\"Time points\", fontsize=20)\n",
    "# ax2 = sns.lineplot(x='index', y='value', hue='variable', data=df_tft_result.loc[df_tft_result['variable'] == 'Target',], palette=palette)\n",
    "conf = ax.fill_between(np.arange(preds_.shape[0]), df_tft_result.loc[df_tft_result['variable'] == '10%', 'value'], df_tft_result.loc[df_tft_result['variable'] == '90%', 'value'], color='purple', alpha=0.3, label=r'80% interval (TFT)')\n",
    "\n",
    "ax1.set_xticks([100 * i -20 for i in range(15)])\n",
    "ax1.set_xticklabels([str(100 * (i - 5)) for i in range(15)] ,fontsize = 14)\n",
    "ax1.set_xlim(480, 780)\n",
    "ax1.set_ylim(0.23, 0.53)\n",
    "ax.legend(loc = 'upper right')    \n",
    "plt.show()\n",
    "\n",
    "ax.legend(loc = 'upper left')    \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
