{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset #IterableDataset\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.abspath('')))\n",
    "sys.path.append(os.path.abspath(os.path.abspath('') + '/src'))\n",
    "os.chdir(os.path.abspath(''))\n",
    "df_mead = pd.read_csv(\"../data/us_lakes/df_mead_preprocessed.csv\")\n",
    "df_mohave = pd.read_csv(\"../data/us_lakes/df_mohave_preprocessed.csv\")\n",
    "df_havasu = pd.read_csv(\"../data/us_lakes/df_havasu_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split_for_dl(df_mead, df_mohave, df_havasu, valid_size=2/9, test_size=1/3, input_seq_len=24, tau=4):\n",
    "    N, _ = df_mohave.shape\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    \n",
    "    df_mead.columns = [\"e1\", \"i1\", \"o1\", \"y\", \"m\", \"d\"]\n",
    "    df_mohave.columns = [\"e2\", \"i2\", \"o2\", \"y\", \"m\", \"d\", \"p1\"]\n",
    "    df_havasu.columns = [\"e3\", \"i3\", \"o3\", \"y\", \"m\", \"d\", \"p2\"]\n",
    "    \n",
    "    df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
    "    df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
    "    df_havasu.drop(columns=[\"y\"], inplace=True)\n",
    "    \n",
    "    df_havasu_date = df_havasu.loc[:, [\"m\", \"d\"]]\n",
    "    df_havasu_cov = df_havasu.loc[:, [\"e3\", \"i3\", \"o3\", \"p2\"]]\n",
    "    \n",
    "    index_1 = round(N * (1 - valid_size - test_size))\n",
    "    index_2 = round(N * (1-test_size))\n",
    "    \n",
    "    df_mead_train = df_mead.iloc[:index_1, :]\n",
    "    df_mohave_train = df_mohave.iloc[:index_1, :]\n",
    "    df_havasu_train = df_havasu_cov.iloc[:index_1, :]\n",
    "    df_train_date = df_havasu_date.iloc[:index_1, :]\n",
    "    \n",
    "    df_mead_valid = df_mead.iloc[index_1:index_2, :]\n",
    "    df_mohave_valid = df_mohave.iloc[index_1:index_2, :]\n",
    "    df_havasu_valid = df_havasu_cov.iloc[index_1:index_2, :]\n",
    "    df_valid_date = df_havasu_date.iloc[index_1:index_2, :]\n",
    "\n",
    "    df_mead_test = df_mead.iloc[index_2:, :]\n",
    "    df_mohave_test = df_mohave.iloc[index_2:, :]\n",
    "    df_havasu_test = df_havasu_cov.iloc[index_2:, :]\n",
    "    df_test_date = df_havasu_date.iloc[index_2:, :]\n",
    "    \n",
    "    df_train = pd.concat([df_mead_train, df_mohave_train, df_havasu_train], axis=1)\n",
    "    df_valid = pd.concat([df_mead_valid, df_mohave_valid, df_havasu_valid], axis=1)\n",
    "    df_test = pd.concat([df_mead_test, df_mohave_test, df_havasu_test], axis=1)\n",
    "    \n",
    "    imp_mean.fit(df_train)\n",
    "    df_train = imp_mean.transform(df_train)\n",
    "    df_valid = imp_mean.transform(df_valid)\n",
    "    df_test = imp_mean.transform(df_test)\n",
    "    \n",
    "    col_labels = 7\n",
    "    \n",
    "    tmp_arr = np.array(df_train)\n",
    "    tmp_label = np.array(df_train[:, col_labels])\n",
    "    tmp_date = np.array(df_train_date)\n",
    "    \n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    train_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    train_label = np.zeros((n, tau))\n",
    "    train_date = np.zeros((n, input_seq_len, 2))\n",
    "        \n",
    "    for j in range(n):\n",
    "        train_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        train_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        train_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "\n",
    "    tmp_arr = np.array(df_valid)\n",
    "    tmp_label = np.array(df_valid[:, col_labels])\n",
    "    tmp_date = np.array(df_valid_date)\n",
    "    \n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    valid_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    valid_label = np.zeros((n, tau))\n",
    "    valid_date = np.zeros((n, input_seq_len, 2))\n",
    "    \n",
    "    for j in range(n):\n",
    "        valid_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        valid_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        valid_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "        \n",
    "    tmp_arr = np.array(df_test)\n",
    "    tmp_label = np.array(df_test[:, col_labels])\n",
    "    tmp_date = np.array(df_test_date)\n",
    "    \n",
    "    _, p = tmp_arr.shape\n",
    "    n = tmp_arr.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    test_input = np.zeros((n, input_seq_len, p), dtype=np.float32)\n",
    "    test_label = np.zeros((n, tau))\n",
    "    test_date = np.zeros((n, input_seq_len, 2))\n",
    "    \n",
    "    for j in range(n):\n",
    "        test_input[j, :] = tmp_arr[j:(j+input_seq_len)]\n",
    "        test_label[j, :] = tmp_label[(j+input_seq_len):(j+input_seq_len+tau)]/1000\n",
    "        test_date[j, :] = tmp_date[j:(j+input_seq_len)]\n",
    "    \n",
    "    \n",
    "    scaler.fit(train_input.reshape(train_input.shape[0], -1))\n",
    "    train_scaled = scaler.transform(train_input.reshape(train_input.shape[0], -1)).reshape(train_input.shape[0], input_seq_len, -1)\n",
    "    valid_scaled = scaler.transform(valid_input.reshape(valid_input.shape[0], -1)).reshape(valid_input.shape[0], input_seq_len, -1)\n",
    "    test_scaled = scaler.transform(test_input.reshape(test_input.shape[0], -1)).reshape(test_input.shape[0], input_seq_len, -1)\n",
    "    \n",
    "    return (train_scaled, train_date, train_label), (valid_scaled, valid_date, valid_label), (test_scaled, test_date, test_label), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=4, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i \n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class GTCN(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, num_channels, kernel_size=8, dropout=0.1):\n",
    "        super(GTCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(seq_len, seq_len)\n",
    "        self.linear2 = nn.Linear(seq_len, seq_len)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        x1 = self.linear1(x)\n",
    "        x2 = self.sigmoid(self.linear2(x))\n",
    "        \n",
    "        return (x1 * x2).unsqueeze(-2).transpose(3, 1).transpose(3, 2)\n",
    "\n",
    "def nconv(x, A):\n",
    "    return torch.einsum('ncvl,vw->ncwl', (x, A)).contiguous()\n",
    "\n",
    "class GraphConvNet(nn.Module):\n",
    "    def __init__(self, adj, node_dim, dropout):\n",
    "        super(GraphConvNet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.linears = nn.ModuleList([nn.Linear(node_dim, node_dim) for i in range(2)])\n",
    "        self.adj = adj \n",
    "    def forward(self, x):\n",
    "        for _,l in enumerate(self.linears):\n",
    "            x = nconv(l(x), self.adj)\n",
    "        h = F.dropout(x, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "class GraphAttentionNet(nn.Module):\n",
    "    def __init__(self, adj, in_feature, out_feature, dropout, concat=True):\n",
    "        super(GraphAttentionNet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_feature\n",
    "        self.out_features = out_feature\n",
    "        self.concat = concat\n",
    "        self.adj = adj\n",
    "        self.W = nn.Parameter(torch.empty(size=(self.in_features, self.out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*self.out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, h):\n",
    "        Wh = torch.matmul(h, self.W)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(self.adj > 0, e, zero_vec)\n",
    "        alpha_ = F.softmax(attention, dim=-1)\n",
    "        alpha = F.dropout(alpha_, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(alpha, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "        e = Wh1 + Wh2.transpose(3, 2)\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "class GraphFusion(nn.Module):\n",
    "    def __init__(self, adj, node_dim, in_feature, out_feature, dropout):\n",
    "        super(GraphFusion, self).__init__()\n",
    "        self.gcn = GraphConvNet(adj=adj, node_dim=node_dim, dropout=dropout)\n",
    "        self.gat = GraphAttentionNet(adj=adj, in_feature=in_feature, out_feature=out_feature, dropout=dropout)\n",
    "        self.adj = adj\n",
    "    def forward(self, x):\n",
    "        x1 = self.gcn(x)\n",
    "        x2 = self.gat(x)\n",
    "        \n",
    "        return x1 + x2\n",
    "        \n",
    "class HSDSTM(nn.Module):\n",
    "    \"\"\"Deng et al, 2023 (Stochastic Environmental Research and Risk Assessment)\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 adj = None,\n",
    "                 input_size=11,\n",
    "                 seq_len=24,\n",
    "                 num_channels=[11, 11],\n",
    "                 node_dim=1,\n",
    "                 dropout=0.1,\n",
    "                 num_levels=2,\n",
    "                 tau=4,\n",
    "                 num_quantiles=5\n",
    "                 ):\n",
    "        super(HSDSTM, self).__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        \n",
    "        self.levels = nn.ModuleList([nn.Sequential(\n",
    "            GTCN(input_size=input_size, seq_len=seq_len, num_channels=num_channels),\n",
    "            GraphFusion(adj=self.adj, node_dim=node_dim, in_feature=node_dim, out_feature=node_dim, dropout=dropout)\n",
    "        ) for _ in range(num_levels)])\n",
    "        \n",
    "        self.qol = nn.ModuleList([nn.Linear(seq_len, tau) for _ in range(num_quantiles)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output_list = []\n",
    "        \n",
    "        for _, l in enumerate(self.levels):\n",
    "            h = l(x)\n",
    "            h = h + x.transpose(2, 1).unsqueeze(-1)\n",
    "            output_list.append(h)\n",
    "            x = x.squeeze()\n",
    "        \n",
    "        fusion = torch.cat(output_list, dim=-1).mean(dim=-2).mean(dim=-1)\n",
    "        \n",
    "        total_output_list = []\n",
    "        \n",
    "        for _,l in enumerate(self.qol):\n",
    "            tmp_quantile_output = l(fusion)\n",
    "            total_output_list.append(tmp_quantile_output.unsqueeze(-1))\n",
    "        \n",
    "        return torch.cat(total_output_list, dim=-1)\n",
    "    \n",
    "class QuantileRisk(nn.Module):\n",
    "    def __init__(self, tau, quantile, device):\n",
    "        super(QuantileRisk, self).__init__()\n",
    "        self.quantile = quantile\n",
    "        self.device = device\n",
    "        self.q_arr = torch.tensor(quantile).float().unsqueeze(-1).repeat(1, 1, tau).transpose(-1, -2).to(self.device)\n",
    "    \n",
    "    def forward(self, true, pred):\n",
    "        \n",
    "        ql = torch.maximum(self.q_arr * (true.unsqueeze(-1) - pred), (1-self.q_arr)*(pred - true.unsqueeze(-1)))\n",
    "\n",
    "        return ql.mean()\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = []\n",
    "    \n",
    "    for batch in loader:\n",
    "        conti_input, true_y = batch \n",
    "        \n",
    "        conti_input = conti_input.to(device)\n",
    "        true_y = true_y.to(device)\n",
    "        \n",
    "        pred = model(conti_input)\n",
    "        \n",
    "        loss = criterion(true_y, pred)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss)\n",
    "        \n",
    "    return sum(total_loss)/len(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70916/1876199159.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Train Loss: 0.0007: 100%|██████████| 1500/1500 [01:33<00:00, 15.97it/s]\n",
      "/tmp/ipykernel_70916/1876199159.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Train Loss: 0.0005: 100%|██████████| 1500/1500 [01:33<00:00, 15.97it/s]\n",
      "/tmp/ipykernel_70916/1876199159.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Train Loss: 0.0004: 100%|██████████| 1500/1500 [01:35<00:00, 15.72it/s]\n",
      "/tmp/ipykernel_70916/1876199159.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mead.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mohave.drop(columns=[\"y\", \"m\", \"d\"], inplace=True)\n",
      "/tmp/ipykernel_70916/1876199159.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_havasu.drop(columns=[\"y\"], inplace=True)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Train Loss: 0.0004: 100%|██████████| 1500/1500 [01:27<00:00, 17.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data_split_range = [(2005, 2013), (2008, 2016), (2011, 2019), (2014, 2022)]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "adj = torch.tensor([[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], \n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n",
    "                ).float()\n",
    "\n",
    "norm_adj = adj/adj.sum(dim=-1).unsqueeze(-1)\n",
    "norm_adj = norm_adj.to(device)\n",
    "\n",
    "ql_09 = []\n",
    "ql_07 = []\n",
    "ql_05 = []\n",
    "ql_03 = []\n",
    "ql_01 = []\n",
    "\n",
    "qr_09 = []\n",
    "qr_07 = []\n",
    "qr_05 = []\n",
    "qr_03 = []\n",
    "qr_01 = []\n",
    "\n",
    "for a, b in data_split_range:\n",
    "    tmp_train, tmp_valid, tmp_test, scaler = train_valid_test_split_for_dl(df_mead.loc[(df_mead[\"year\"] >= a) & (df_mead[\"year\"] <= b)],\n",
    "                                            df_mohave.loc[(df_mohave[\"year\"] >= a) & (df_mohave[\"year\"] <= b)],\n",
    "                                            df_havasu.loc[(df_havasu[\"year\"] >= a) & (df_havasu[\"year\"] <= b)])\n",
    "     \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(np.swapaxes(tmp_train[0], 2, 1)), torch.FloatTensor(tmp_train[2]))\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=256)\n",
    "\n",
    "    deng = HSDSTM(adj=norm_adj,\n",
    "                 input_size=11,\n",
    "                 seq_len=24,\n",
    "                 num_channels=[11, 11],\n",
    "                 node_dim=1,\n",
    "                 dropout=0.05,\n",
    "                 num_levels=2,\n",
    "                 tau=4,\n",
    "                 num_quantiles=5)\n",
    "    \n",
    "    deng.to(device)\n",
    "    optimizer = optim.AdamW(deng.parameters(), lr=0.001)\n",
    "    criterion = QuantileRisk(4, [0.1, 0.3, 0.5, 0.7, 0.9], device)\n",
    "    \n",
    "    pbar = tqdm(range(1500))\n",
    "\n",
    "    for epoch in pbar:        \n",
    "        train_loss = train(deng, train_loader, criterion, optimizer, device)\n",
    "        pbar.set_description(\"Train Loss: {:.4f}\".format(train_loss))\n",
    "    \n",
    "    torch.save(deng.state_dict(), f'../assets/us_lakes/HSDSTM_us_lakes_{a}_{b}.pth')\n",
    "    \n",
    "    test_input = torch.FloatTensor(np.swapaxes(tmp_test[0], 2, 1)).to(device)\n",
    "    label = tmp_test[2]\n",
    "    \n",
    "    deng.eval()    \n",
    "    with torch.no_grad():\n",
    "        pred_results = deng(test_input)\n",
    "        pred_results = pred_results.detach().cpu().numpy()\n",
    "    \n",
    "    ql_09.append(np.maximum(0.9 * (label - pred_results[..., 4]), (1-0.9)*(pred_results[..., 4] - label)).mean() * 1000)\n",
    "    ql_05.append(np.maximum(0.5 * (label - pred_results[..., 2]), (1-0.5)*(pred_results[..., 2] - label)).mean() * 1000)\n",
    "    ql_01.append(np.maximum(0.1 * (label - pred_results[..., 0]), (1-0.1)*(pred_results[..., 0] - label)).mean() * 1000)\n",
    "        \n",
    "    qr_09.append((np.mean(label < pred_results[..., 4]), 0.9 - np.mean(label < pred_results[..., 4])))\n",
    "    qr_05.append((np.mean(label < pred_results[..., 2]), 0.5 - np.mean(label < pred_results[..., 2])))\n",
    "    qr_01.append((np.mean(label < pred_results[..., 0]), 0.1 - np.mean(label < pred_results[..., 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.181"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ql_09).mean().round(3)\n",
    "np.array(ql_05).mean().round(3)\n",
    "np.array(ql_01).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_09]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_09]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_05]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_05]).mean().round(3)\n",
    "\n",
    "np.array([x for x, _ in qr_01]).mean().round(3)\n",
    "np.array([np.abs(x) for _, x in qr_01]).mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
