{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.arima import ARIMA, AutoARIMA\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, ValueWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.simplefilter('ignore', ValueWarning)\n",
    "\n",
    "\n",
    "os.chdir(os.path.abspath(''))\n",
    "df_train_total = pd.read_csv(\"../data/df_train_total.csv\")\n",
    "df_test_total = pd.read_csv(\"../data/df_test_total.csv\")\n",
    "df_merged = pd.read_csv(\"../data/df_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds_arima(label_df, input_seq_len=48, tau=12):\n",
    "    col_labels =  'wl_1018680' # ['wl_1018662', 'wl_1018680', 'wl_1018683', 'wl_1019630']\n",
    "    \n",
    "    tmp_df = np.array(label_df.loc[label_df['month'].isin([2,3]), col_labels])\n",
    "    \n",
    "    n = tmp_df.shape[0] - input_seq_len - tau \n",
    "    \n",
    "    conti_input = np.zeros((n, input_seq_len), dtype=np.float32)\n",
    "    label = np.zeros((n, tau))\n",
    "\n",
    "    for j in range(n):\n",
    "        conti_input[j, :] = tmp_df[j:(j+input_seq_len)]\n",
    "        label[j, :] = tmp_df[(j+input_seq_len):(j+input_seq_len+tau)]\n",
    "\n",
    "    return conti_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_05_ql_results, arima_07_ql_results, arima_09_ql_results, arima_05_qr_results, arima_07_qr_results, arima_09_qr_results = [], [], [], [], [], []\n",
    "theta_05_ql_results, theta_07_ql_results, theta_09_ql_results, theta_05_qr_results, theta_07_qr_results, theta_09_qr_results = [], [], [], [], [], []\n",
    "lightgbm_05_ql_results, lightgbm_07_ql_results, lightgbm_09_ql_results, lightgbm_05_qr_results, lightgbm_07_qr_results, lightgbm_09_qr_results = [], [], [], [], [], []\n",
    "ets_05_ql_results, ets_07_ql_results, ets_09_ql_results, ets_05_qr_results, ets_07_qr_results, ets_09_qr_results = [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "    df_train_total = pd.read_csv(\"../data/df_train_total_ds_{}.csv\".format(year))\n",
    "    df_test_total = pd.read_csv(\"../data/df_test_total_ds_{}.csv\".format(year))\n",
    "    df_merged = pd.read_csv(\"../data/df_merged_ds_{}.csv\".format(year))\n",
    "\n",
    "    fh = ForecastingHorizon(np.arange(1, 13), is_relative=True)\n",
    "\n",
    "    arima_forecaster = ARIMA()\n",
    "\n",
    "    arima_results = []\n",
    "\n",
    "    y_train = df_merged.loc[df_merged[\"month\"].isin([0, 1]), \"wl_1018680\"]\n",
    "    arima_forecaster.fit(y_train)\n",
    "\n",
    "    y_input, label = generate_ds_arima(df_merged)\n",
    "\n",
    "    for i in range(y_input.shape[0]):\n",
    "        arima_forecaster.update(y_input[i], update_params=False)\n",
    "        tmp_result = arima_forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        arima_results.append(tmp_result.values[np.newaxis, ...])\n",
    "\n",
    "    arima_results = np.concatenate(arima_results, axis=0)\n",
    "\n",
    "    arima_09_ql_results.append(np.maximum(0.9 * (label - arima_results[..., 3]), (1-0.9)*(arima_results[..., 3] -label )).mean()/1000)\n",
    "    arima_07_ql_results.append(np.maximum(0.7 * (label - arima_results[..., 2]), (1-0.7)*(arima_results[..., 2] -label )).mean()/1000)\n",
    "    arima_05_ql_results.append(np.maximum(0.5 * (label - arima_results[..., 1]), (1-0.5)*(arima_results[..., 1] -label )).mean()/1000)\n",
    "    \n",
    "    arima_09_qr_results.append([np.mean(label < arima_results[..., 3]), np.abs(0.9 - np.mean(label < arima_results[..., 3]))])\n",
    "    arima_07_qr_results.append([np.mean(label < arima_results[..., 2]), np.abs(0.7 - np.mean(label < arima_results[..., 2]))])\n",
    "    arima_05_qr_results.append([np.mean(label < arima_results[..., 1]), np.abs(0.5 - np.mean(label < arima_results[..., 1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(arima_09_ql_results).round(4),\n",
    "    np.mean(arima_07_ql_results).round(4),\n",
    "    np.mean(arima_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(arima_09_qr_results, axis=0).round(4),\n",
    "    np.mean(arima_07_qr_results, axis=0).round(4),\n",
    "    np.mean(arima_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb \n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "def generate_ts_data_train_for_lgb(label_df, term_list, input_seq_len=48, tau=12):\n",
    "    conti_input_list = []\n",
    "    label_list = []\n",
    "    col_labels =  ['wl_1018680'] \n",
    "\n",
    "    for i in label_df['year'].unique():\n",
    "        tmp_df = np.array(label_df.loc[label_df['month'].isin([0, 1]), :])\n",
    "        tmp_label_df = np.array(label_df.loc[label_df['month'].isin([0, 1]), col_labels])        \n",
    "        n = tmp_df.shape[0] - input_seq_len - tau \n",
    "        covariate = np.zeros((n, input_seq_len, tmp_df.shape[1] - 4 ))   \n",
    "        label = np.zeros((n, tau, len(col_labels)))\n",
    "        \n",
    "        for j in range(n):\n",
    "            covariate[j, :, :] = tmp_df[j:(j+input_seq_len), 4:]\n",
    "            label[j, :, :] = tmp_label_df[(j+input_seq_len):(j+input_seq_len+tau), :]/1000\n",
    "\n",
    "        conti_input_list.append(covariate)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    total_conti_input = np.concatenate(conti_input_list, axis=0)\n",
    "    total_label = np.concatenate(label_list, axis=0)\n",
    "    \n",
    "    return total_conti_input.reshape(-1, total_conti_input.shape[1] * total_conti_input.shape[2]), np.squeeze(total_label)\n",
    "\n",
    "def generate_ts_data_test_for_lgb(label_df, term_list, input_seq_len=48, tau=12):\n",
    "    conti_input_list = []\n",
    "    label_list = []\n",
    "    col_labels =  ['wl_1018680'] \n",
    "    \n",
    "    for i in label_df['year'].unique():\n",
    "        tmp_df = np.array(label_df.loc[label_df['month'].isin([2, 3]), :])\n",
    "        tmp_label_df = np.array(label_df.loc[label_df['month'].isin([2, 3]), col_labels])\n",
    "        n = tmp_df.shape[0] - input_seq_len - tau \n",
    "    \n",
    "        covariate = np.zeros((n, input_seq_len, tmp_df.shape[1] - 4 ))     \n",
    "        label = np.zeros((n, tau, len(col_labels)))\n",
    "\n",
    "        for j in range(n):\n",
    "            covariate[j, :, :] = tmp_df[j:(j+input_seq_len), 4:]\n",
    "            label[j, :, :] = tmp_label_df[(j+input_seq_len):(j+input_seq_len+tau), :]/1000\n",
    "\n",
    "        conti_input_list.append(covariate)\n",
    "        label_list.append(label)\n",
    "    \n",
    "    total_conti_input = np.concatenate(conti_input_list, axis=0)\n",
    "    total_label = np.concatenate(label_list, axis=0)\n",
    "    \n",
    "    return total_conti_input.reshape(-1, total_conti_input.shape[1] * total_conti_input.shape[2]), np.squeeze(total_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.fourier import FourierFeatures\n",
    "\n",
    "for year in [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "\n",
    "    df_train_total = pd.read_csv(\"../data/df_train_total_ds_{}.csv\".format(year))\n",
    "    df_test_total = pd.read_csv(\"../data/df_test_total_ds_{}.csv\".format(year))\n",
    "    df_merged = pd.read_csv(\"../data/df_merged_ds_{}.csv\".format(year))\n",
    "\n",
    "    term_list = 4\n",
    "    transformer = FourierFeatures(sp_list=[24], fourier_terms_list=[term_list])\n",
    "\n",
    "    df_merged_lgb = pd.concat([df_merged, transformer.fit_transform(df_merged[[\"wl_1018680\"]])], axis=1)\n",
    "    train_input_lgb, label_lgb = generate_ts_data_train_for_lgb(df_merged_lgb, term_list)\n",
    "\n",
    "    alphas = [0.5, 0.7, 0.9]\n",
    "\n",
    "    model_list = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        params = {\n",
    "            \"objective\": \"quantile\",\n",
    "            \"alpha\": alpha,\n",
    "            \"boosting\": \"gbdt\",\n",
    "\n",
    "        }\n",
    "        gbm = lgb.LGBMRegressor(**params)\n",
    "        regr_multiglb = MultiOutputRegressor(gbm)\n",
    "        regr_multiglb.fit(train_input_lgb, label_lgb)\n",
    "        \n",
    "        model_list.append(\n",
    "            regr_multiglb\n",
    "        )\n",
    "\n",
    "    test_input_lgb, test_label_lgb = generate_ts_data_test_for_lgb(df_merged_lgb, term_list)\n",
    "    test_input_lgb.shape\n",
    "\n",
    "    lgb_results_05 = model_list[0].predict(test_input_lgb)\n",
    "    lgb_results_07 = model_list[1].predict(test_input_lgb)\n",
    "    lgb_results_09 = model_list[2].predict(test_input_lgb)\n",
    "\n",
    "    lightgbm_09_ql_results.append(np.maximum(0.9 * (test_label_lgb - lgb_results_09), (1-0.9)*(lgb_results_09 - test_label_lgb)).mean())\n",
    "    lightgbm_07_ql_results.append(np.maximum(0.7 * (test_label_lgb - lgb_results_07), (1-0.7)*(lgb_results_07 - test_label_lgb)).mean())\n",
    "    lightgbm_05_ql_results.append(np.maximum(0.5 * (test_label_lgb - lgb_results_05), (1-0.5)*(lgb_results_05 - test_label_lgb)).mean())\n",
    "\n",
    "    lightgbm_09_qr_results.append([np.mean(test_label_lgb < lgb_results_09), np.abs(0.9 - np.mean(test_label_lgb < lgb_results_09))])\n",
    "    lightgbm_07_qr_results.append([np.mean(test_label_lgb < lgb_results_07), np.abs(0.7 - np.mean(test_label_lgb < lgb_results_07))])\n",
    "    lightgbm_05_qr_results.append([np.mean(test_label_lgb < lgb_results_05), np.abs(0.5 - np.mean(test_label_lgb < lgb_results_05))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(lightgbm_09_ql_results).round(4),\n",
    "    np.mean(lightgbm_07_ql_results).round(4),\n",
    "    np.mean(lightgbm_05_ql_results).round(4)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(lightgbm_09_qr_results, axis=0).round(4),\n",
    "    np.mean(lightgbm_07_qr_results, axis=0).round(4),\n",
    "    np.mean(lightgbm_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "    df_train_total = pd.read_csv(\"../data/df_train_total_ds_{}.csv\".format(year))\n",
    "    df_test_total = pd.read_csv(\"../data/df_test_total_ds_{}.csv\".format(year))\n",
    "    df_merged = pd.read_csv(\"../data/df_merged_ds_{}.csv\".format(year))\n",
    "\n",
    "    fh = ForecastingHorizon(np.arange(1, 13), is_relative=True)\n",
    "\n",
    "    y_train = df_merged.loc[df_merged[\"month\"].isin([0, 1]), \"wl_1018680\"]\n",
    "    y_input, label = generate_ds_arima(df_merged)\n",
    "\n",
    "    ets_results = []\n",
    "    ets_forecaster = AutoETS(auto=True, n_jobs=-1)  \n",
    "\n",
    "    ets_forecaster.fit(y_train)\n",
    "\n",
    "    for i in range(y_input.shape[0]):\n",
    "        ets_forecaster.update(y_input[i], update_params=False)\n",
    "        tmp_result = ets_forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        ets_results.append(tmp_result.values[np.newaxis, ...])\n",
    "        \n",
    "    ets_results = np.concatenate(ets_results, axis=0)\n",
    "\n",
    "    ets_09_ql_results.append(np.maximum(0.9 * (label - ets_results[..., 3]), (1-0.9)*(ets_results[..., 3] -label )).mean()/1000)\n",
    "    ets_07_ql_results.append(np.maximum(0.7 * (label - ets_results[..., 2]), (1-0.7)*(ets_results[..., 2] -label )).mean()/1000)\n",
    "    ets_05_ql_results.append(np.maximum(0.5 * (label - ets_results[..., 1]), (1-0.5)*(ets_results[..., 1] -label )).mean()/1000)\n",
    "    \n",
    "    ets_09_qr_results.append([np.mean(label < ets_results[..., 3]), np.abs(0.9 - np.mean(label < ets_results[..., 3]))])\n",
    "    ets_07_qr_results.append([np.mean(label < ets_results[..., 2]), np.abs(0.7 - np.mean(label < ets_results[..., 2]))])\n",
    "    ets_05_qr_results.append([np.mean(label < ets_results[..., 1]), np.abs(0.5 - np.mean(label < ets_results[..., 1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(ets_09_ql_results).round(4),\n",
    "    np.mean(ets_07_ql_results).round(4),\n",
    "    np.mean(ets_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(ets_09_qr_results, axis=0).round(4),\n",
    "    np.mean(ets_07_qr_results, axis=0).round(4),\n",
    "    np.mean(ets_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]:\n",
    "    df_train_total = pd.read_csv(\"../data/df_train_total_ds_{}.csv\".format(year))\n",
    "    df_test_total = pd.read_csv(\"../data/df_test_total_ds_{}.csv\".format(year))\n",
    "    df_merged = pd.read_csv(\"../data/df_merged_ds_{}.csv\".format(year))\n",
    "\n",
    "    fh = ForecastingHorizon(np.arange(1, 13), is_relative=True)\n",
    "\n",
    "    y_train = df_merged.loc[df_merged[\"month\"].isin([0, 1]), \"wl_1018680\"]\n",
    "    y_input, label = generate_ds_arima(df_merged)\n",
    "\n",
    "    theta_results = []\n",
    "    theta_forecaster = ThetaForecaster()    \n",
    "\n",
    "    theta_forecaster.fit(y_train)\n",
    "\n",
    "    for i in range(y_input.shape[0]):\n",
    "        theta_forecaster.update(y_input[i], update_params=False)\n",
    "        tmp_result = theta_forecaster.predict_quantiles(fh=fh, alpha=[0.1, 0.5, 0.7, 0.9])\n",
    "        \n",
    "        theta_results.append(tmp_result.values[np.newaxis, ...])\n",
    "\n",
    "    theta_results = np.concatenate(theta_results, axis=0)\n",
    "\n",
    "    theta_09_ql_results.append(np.maximum(0.9 * (label - theta_results[..., 3]), (1-0.9)*(theta_results[..., 3] -label )).mean()/1000)\n",
    "    theta_07_ql_results.append(np.maximum(0.7 * (label - theta_results[..., 2]), (1-0.7)*(theta_results[..., 2] -label )).mean()/1000)\n",
    "    theta_05_ql_results.append(np.maximum(0.5 * (label - theta_results[..., 1]), (1-0.5)*(theta_results[..., 1] -label )).mean()/1000)\n",
    "    \n",
    "    theta_09_qr_results.append([np.mean(label < theta_results[..., 3]), np.abs(0.9 - np.mean(label < theta_results[..., 3]))])\n",
    "    theta_07_qr_results.append([np.mean(label < theta_results[..., 2]), np.abs(0.7 - np.mean(label < theta_results[..., 2]))])\n",
    "    theta_05_qr_results.append([np.mean(label < theta_results[..., 1]), np.abs(0.5 - np.mean(label < theta_results[..., 1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(theta_09_ql_results).round(4),\n",
    "    np.mean(theta_07_ql_results).round(4),\n",
    "    np.mean(theta_05_ql_results).round(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    np.mean(theta_09_qr_results, axis=0).round(4),\n",
    "    np.mean(theta_07_qr_results, axis=0).round(4),\n",
    "    np.mean(theta_05_qr_results, axis=0).round(4),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
